{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Ap≈°valka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) with edits by [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/) and Mohamed Abdelhack\n",
    "- **Date:** November 2024\n",
    "- **conda environment**: This uses the [mri environment](https://github.com/RikHenson/PythonNeuroimagingCourse/blob/main/mri_environment.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis: Group-Level Analysis\n",
    "\n",
    "Once you have beta (or contrast) maps for conditions (or contrasts) from all subjects, you can perform group-level statistics. Importantly, all subject first-level results need to be in common space, e.g., MNI, to perform voxel-wise group analyses. Group-level analysis allows you to make inferences about the population, rather than individual subjects, by assessing common activations across participants. Common statistical methods for group-level analysis include one-sample or paired t-tests, as well as more complex ANOVA models, depending on your study design.\n",
    "\n",
    "In this tutorial, we illustrate a simple one-sample t-test on the contrast of Faces-Scrambled that we calculated for each subject in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import required packages and set up some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conda environment used for this tutorial is available here: https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml \n",
    "import os\n",
    "import glob   # to search for files using regex\n",
    "import pandas # for data manipulation\n",
    "import numpy  # for numerical operations\n",
    "\n",
    "from bids.layout import BIDSLayout # to fetch data from BIDS-compliant datasets\n",
    "import matplotlib.pyplot as plt # for basic plotting\n",
    "\n",
    "import nibabel  # to read and write neuroimaging data, https://nipy.org/nibabel/\n",
    "\n",
    "# Nilearn modules, for the analysis of brain volumes, plotting, etc., https://nilearn.github.io/\n",
    "from nilearn.plotting import plot_glass_brain, plot_design_matrix, plot_contrast_matrix, plot_stat_map, view_img, view_img_on_surf\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "from nilearn.datasets import load_mni152_template\n",
    "# from nilearn.glm.second_level import non_parametric_inference\n",
    "\n",
    "\n",
    "# to show plots in cell\n",
    "%matplotlib inline   \n",
    "\n",
    "wd = '/home/mohamed/Documents/ISN2025/FaceRecognition' # <-- CHANGE TO YOURS\n",
    "os.chdir(wd)\n",
    "print(f\"Working directory currently {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNI152 template will be used as a backgound for plotting results\n",
    "mni152_template = load_mni152_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve First-Level results\n",
    "\n",
    "For the group analysis, we will use the single-condition contrast estimate (beta estimate) maps for all nine conditions. Because we saved the results in BIDS format, we can us PyBIDS to retrieve the subject-level results. \n",
    "\n",
    "Note that if you did not run first-level models for all subjects, you can download the contrast images needed from the `results/first-level` directory from [https://cloud.mrc-cbu.cam.ac.uk/index.php/s/3AFA0yF7QK9qIaO](https://cloud.mrc-cbu.cam.ac.uk/index.php/s/3AFA0yF7QK9qIaO). The password for this cloud directory will be given by the course organiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up the paths to the data and results folders\n",
    "fmri_data_dir = 'data' # data in BIDS format\n",
    "fmri_results_dir = 'results' # results in BIDS format\n",
    "\n",
    "fmri_group_dir = os.path.join(fmri_results_dir, 'group-level') # where group results will go\n",
    "try: \n",
    "    os.mkdir(fmri_group_dir)\n",
    "except: None\n",
    "\n",
    "# --- Set up the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# Attach the results folder to the layout. It must comply with BIDS standards, and include dataset_description.json file!\n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))\n",
    "subjects = layout.get_subjects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Specify which conditions to include in the analysis and retrieve their effect files from the first-level results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Z-maps for each subject\n",
    "\n",
    "To check how the first-level results look overall, it is helpful to display the Faces-Scrambled contrast for all subjects. Let's find these and then plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac_scr_maps = layout.get(desc='FacesScrambled', suffix=\"effect\", extension='.nii.gz', return_type='file')\n",
    "print(*fac_scr_maps, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(14, 14));\n",
    "\n",
    "for i, stat_map in enumerate(fac_scr_maps):\n",
    "    plot_glass_brain(stat_map, title = 'sub-' + subjects[i],\n",
    "                               axes = axes[int(i / 4), int(i % 4)],\n",
    "                               plot_abs = False, \n",
    "                               display_mode='x')\n",
    "fig.suptitle('Faces > Scrambled' + ', unthresholded z-maps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify second-level models\n",
    "\n",
    "At the group-level analysis, we also use a GLM. The outcome variable is the beta/contrast estimate from each subject, and the predictor variables typically include group-level factors such as experimental conditions, subject-specific regressors (in repeated-measures designs), group-specific regressors (in between-subject designs), and subject-specific covariates (e.g., age, gender, or behavioural scores). First we start with the simplest of all GLMs: the one-sample T-test..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sample T-test for Faces > Scrambled\n",
    "\n",
    "One approach is to define all one's contrasts in the first-level (individual subject) models, and take these to the second-level. When those contrasts are all T-contrasts (one-dimensional), the second level model corresponds to a one-sample T-test, i.e, testing whether those contrasts are significantly above or below zero when averaged across participants. This is called the \"partitioned error\" or \"summary statistic\" approach (such that the error in the one-sample t-test corresponds to the contrast x subject interaction in a repeated-measures ANOVA). With this approach, one has to estimate a new second-level model for each contrast of interest. See Notebook 00 for more information (e.g, how to handle F-contrasts in such cases).\n",
    "\n",
    "Later we will try the alternative approach, which is to take the Betas (averaged across runs) for all of the individual conditions into a larger ANOVA model at the second-level (in our case, all 9 conditions, corresponding to a 3x3 ANOVA). This is called the \"pooled error\" approach. This can be more powerful, and simpler, because all one's contrasts can be tested within a single model. However, it makes stronger assumptions about the sphericity of the error, which may be violated, which is why the above \"partitioned error\" approach is generally safer. Again, see Notebook 00 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = pandas.DataFrame([1]*len(subjects), index=subjects, columns=[\"fac-scr\"])\n",
    "second_level_model = SecondLevelModel() \n",
    "second_level_model = second_level_model.fit(\n",
    "  fac_scr_maps, \n",
    "  design_matrix = design_matrix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search the brain for voxels showing more activation for faces than scrambled faces. We will start by using an uncorrected threshold of p<001 (later we will illustrate some corrected thresholds):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-hair shows a cluster in a right mid-fusiform region, close to what is sometimes called the \"Fusiform Face Area\" (FFA). Also activated (in orange) are bilateral anterior medial temporal lobe regions (around amygdala), as well as more posterior bilateral occipital regions (sometimes called OFA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = {'FacesScrambled': [1]}\n",
    "z_map = second_level_model.compute_contrast(contrasts['FacesScrambled'], output_type=\"z_score\")\n",
    "thresholded_map_fpr, threshold_fpr = threshold_stats_img(\n",
    "    z_map, \n",
    "    alpha=0.001, \n",
    "    height_control=None, \n",
    "    cluster_threshold=0,\n",
    "    two_sided=True)\n",
    "\n",
    "plot_stat_map(\n",
    "    thresholded_map_fpr,\n",
    "    bg_img = mni152_template, \n",
    "    threshold = threshold_fpr,   \n",
    "    display_mode = 'ortho',\n",
    "    black_bg = True,    \n",
    "    cut_coords=(41.5, -48.5, -18.5), # to put cross-hair on right FFA (to match ANOVA results below)\n",
    "title = 'Faces > Scrambled')\n",
    "fig = plt.gcf(); fig.set_size_inches(10,3); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "337px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "874.85px",
    "left": "2183px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
