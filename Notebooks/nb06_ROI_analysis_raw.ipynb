{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** [Dace Apšvalka](https://www.mrc-cbu.cam.ac.uk/people/dace.apsvalka/) with edits by [Rik Henson](https://www.mrc-cbu.cam.ac.uk/people/rik.henson/)\n",
    "- **Date:** November 2024\n",
    "- **conda environment**: This uses the [mri environment](https://github.com/RikHenson/PythonNeuroimagingCourse/blob/main/mri_environment.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Analysis: Region Of Interest (ROI) Analysis\n",
    "\n",
    "Along with whole-brain analysis, it is common to perform region-of-interest (ROI) analysis on fMRI data. This involves extracting signals from specified ROIs. ROI analysis is valuable for several reasons, including **data exploration**, **statistical control**, and **functional specification**. For more information, see this brief review paper by R. Poldrack (2007): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2555436/.\n",
    "\n",
    "ROIs can be defined based on brain structure or function. For **structural ROIs**, it is recommended to define them based on each subject's anatomy wherever possible. However, this is not always possible, or feasible. In such cases, one can use ROIs based on probabilistic or (less preferable) deterministic atlases. **Functional ROIs** can be defined from 'localiser' scans that identify specific regions exhibiting a particular response (e.g., 'face area' in the fusiform gyrus that respond more strongly to faces than to non-faces). Alternatively, functional ROIs can be defined using **independent contrasts** (make sure you don't do ['double dipping'](https://www.nature.com/articles/nn.2303)) within a factorial design or derived **from previous studies**.\n",
    "\n",
    "In this tutorial, we demonstrate two common types of ROIs: a sphere around a peak voxel from a functional contrast, and an anatomical ROI from a probabilistic atlas.\n",
    "\n",
    "1. **A spherical ROI** can help to average out noise and increase the reliability of the signal, as well as generalisability across studies. However, averaging across multiple voxels can reduce anatomical and functional specificity, potentially blurring the precise location of the activity.\n",
    "\n",
    "2. **A probabilistic atlas** is commonly used when investigating brain regions that are functionally or anatomically well-characterised in the literature and show consistent properties across individuals. Using a probabilistic atlas allows for more straightforward comparisons between studies, as the ROIs are defined using the same criteria across different datasets. This enhances the reproducibility and comparability of results. Probabilistic atlases help reduce subjective decisions about where to place ROIs, thereby minimising experimenter bias. Unlike deterministic atlases (which are also commonly used), probabilistic atlases account for some variability in the exact location across subjects. However, they still represent an average across individuals, which might not fully capture the specific anatomy or function of a region in a particular subject. \n",
    "\n",
    "It is common, and often advisable, to perform **ROI analysis in a subject's native space** (using their original brain images without normalisation). Nonetheless, we will also provide an example of how to transform an MNI-space ROI to a subject's native space for native-space analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**    \n",
    "1. Import required packages and set up some stuff   \n",
    "2. Load the subject-level fMRI results    \n",
    "3. A single voxel ROI    \n",
    "3.1. Function `extract_MNI_voxel_data`    \n",
    "4. A spherical ROI  \n",
    "4.1. Function `create_voxel_sphere`   \n",
    "4.2. Function `create_MNIvoxel_sphere`   \n",
    "4.3. Function `extract_mean_ROI_values`    \n",
    "5. A ROI from a probabilistic atlas   \n",
    "6. Transforming MNI-space ROI to native-space for native-space analysis    \n",
    "7. Statistics and plotting of the ROI data    \n",
    "7.1. Paired t-test: Initial vs Repeated Faces in the Right Fussiform Gyrus    \n",
    "7.2. 3-way RM ANOVA. Initial vs Immediate Repetition vs Delayed Repetition in the Right Fusiform Gyrus    \n",
    "7.2.1. A simple plot    \n",
    "7.2.2. A 'publication-ready' plot    \n",
    "7.3. 2-by-2 RM ANOVA. Repetition effect for Familiarity x Lag in the Right Fusiform Gurys    \n",
    "7.4. Familiar vs Unfamiliar Faces in Amygdala    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import required packages and set up some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conda environment used for this tutorial is available here: https://github.com/MRC-CBU/COGNESTIC/blob/main/mri_environment.yml \n",
    "\n",
    "import os\n",
    "import os.path # for file path operations\n",
    "import pandas  # for data manipulation\n",
    "import numpy   # for numerical operations\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn # better looking plots\n",
    "import matplotlib.pyplot as plt # basic plots\n",
    "# to show plots in cell\n",
    "%matplotlib inline   \n",
    "\n",
    "# BIDS library\n",
    "from bids.layout import BIDSLayout # to query BIDS dataset\n",
    "\n",
    "# Stats libraries\n",
    "from scipy import stats \n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# # Nilearn modules https://nilearn.github.io/\n",
    "import nilearn\n",
    "from nilearn.datasets import load_mni152_template\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.image import resample_to_img, math_img\n",
    "from nilearn.masking import _unmask_3d, compute_brain_mask\n",
    "from nilearn.maskers import nifti_spheres_masker\n",
    "\n",
    "import nibabel # NiBabel, to read and write neuroimaging data, https://nipy.org/nibabel/\n",
    "import ants    # ANTsPy (Advanced Normalization Tools), https://github.com/ANTsX/ANTsPy\n",
    "\n",
    "wd = '/mnt/c/Users/rh01/PythonNeuroimagingCourse/FaceRecognition/' # <-- CHANGE TO YOURS\n",
    "os.chdir(wd)\n",
    "print(f\"Working directory currently {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNI152 template will be used as a backgound for plotting MNI-space ROIs\n",
    "mni152_template = load_mni152_template() \n",
    "# Note that I could just use '= load_mni152_template()' but I want to easily see where the modules are coming from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the subject-level fMRI results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up the paths to the data and results folders\n",
    "fmri_data_dir = 'data' # data in BIDS format\n",
    "fmri_results_dir = 'results' # results in BIDS format\n",
    "\n",
    "# --- Set up the BIDS layout\n",
    "layout = BIDSLayout(fmri_data_dir, derivatives = True)\n",
    "\n",
    "# Attach the results folder to the layout. It must complay with BIDS standards. \n",
    "# And must include dataset_description.json file!\n",
    "layout.add_derivatives(os.path.join(fmri_results_dir, \"first-level\"))\n",
    "\n",
    "# --- Get all subject-level effect files for the specified conditions\n",
    "conditions = ['IniFF', 'ImmFF', 'DelFF', 'IniUF', 'ImmUF', 'DelUF', 'IniSF', 'ImmSF', 'DelSF']\n",
    "effect_files = layout.get(desc=conditions, suffix='effect', extension='.nii.gz')\n",
    "\n",
    "# Print how many files there are to check if it is correct\n",
    "nsub = len(layout.get_subjects())\n",
    "ncond = len(conditions)\n",
    "print(f\"\\nFound {len(effect_files)} files. Should be {nsub} subjects * {ncond} conditions = {nsub * ncond}\\n\")\n",
    "# Print them for inspection\n",
    "# print(*effect_files, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function to extract the contrast values of the specified voxel from the subject-level results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A functional ROI from a sphere around functional peak (in right fusiform)\n",
    "\n",
    "In this section, we use two functions. The first function calls NiLearn's `create_MNIvoxel_sphere` to create a mask image of voxels within a sphere of a certain radius (in mm) around coordinates that are provided, which should be in MNI-space (or another world-space). The second function, `extract_mean_ROI_values` is used to extract and compute the mean values across voxels within this ROI, for each of one or more fMRI images provided. \n",
    "\n",
    "### Define the spherical mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MNIvoxel_sphere(base_image, center_voxel, radius_mm):\n",
    "    \"\"\"\n",
    "    Create a sphere mask based on the MNI coordinates and radius within the base_image.\n",
    "\n",
    "    Parameters:\n",
    "    - base_image (nibabel.Nifti1Image): A 3D Nifti1Image (in MNI space) that defines the space in which the sphere will be created.\n",
    "    - center_voxel (tuple): The (x, y, z) coordinates of the center voxel, in MNI space.\n",
    "    - radius_mm (float): Radius of the sphere in millimeters.\n",
    "\n",
    "    Returns:\n",
    "    - sphere_mask (nibabel.Nifti1Image): A Nifti1Image containing the sphere mask.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute a brain mask using the povided base image\n",
    "    space_defining_image = compute_brain_mask(base_image)\n",
    "\n",
    "    # Apply mask and get affinity matrix\n",
    "    _, A = nifti_spheres_masker._apply_mask_and_get_affinity(\n",
    "        seeds=[center_voxel],  # in MNI space; can be a list of coordinates\n",
    "        niimg=None,\n",
    "        radius=radius_mm,\n",
    "        allow_overlap=False, \n",
    "        mask_img=space_defining_image\n",
    "    )\n",
    "\n",
    "    # Unmask the sphere mask\n",
    "    sphere_mask_data = _unmask_3d(\n",
    "        X=A.toarray().flatten(), \n",
    "        mask=space_defining_image.get_fdata().astype(bool)\n",
    "    )\n",
    "\n",
    "    # Create a Nifti1Image for the sphere mask\n",
    "    sphere_mask = nibabel.Nifti1Image(sphere_mask_data, space_defining_image.affine)\n",
    "\n",
    "    return sphere_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sphere centre, we will take the MNI coordinates for right fusiform peak from group-level `nilearn_report.html` created in previous notebook; more specifically, the second peak of the Faces > Scrambled one-sample T-test (first peak is occipital face area):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_label = 'Right Fusiform Gyrus'\n",
    "MNI_coord = (41.5,\t-48.5,\t-18.5)\n",
    "\n",
    "# Center voxel in MNI-space\n",
    "center_voxel = MNI_coord\n",
    "\n",
    "# Radius in mm\n",
    "radius = 10\n",
    "\n",
    "# Define the brain mask\n",
    "brain_mask_file = layout.get(return_type='file', datatype='func', suffix='mask', desc='brain', space='MNI152NLin2009cAsym', extension='nii.gz')[0]\n",
    "brain_mask = nibabel.load(brain_mask_file)\n",
    "\n",
    "# Create the sphere\n",
    "sphere_mask = create_MNIvoxel_sphere(brain_mask, center_voxel, radius)\n",
    "\n",
    "# Plot the sphere mask\n",
    "plot_roi(sphere_mask, bg_img=mni152_template, title='Sphere mask', display_mode='ortho', cmap='Set1')\n",
    "\n",
    "# Save the sphere mask to a file\n",
    "sphere_mask_filename = os.path.join(fmri_results_dir,'sphere_mask.nii.gz')\n",
    "nibabel.save(sphere_mask, sphere_mask_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this sphere likely includes white matter (and possibly some CSF). If we want the voxels to be responsive to faces, we can further refine the voxels by those that were above some threshold in the faces > scrambled contrast (a \"functional\" definition) that we created in the second-level, one-sample T-test in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac_scr_thr = nibabel.load(os.path.join(fmri_results_dir,'group-level','faces-scrambled_Zmap_fdr.nii.gz'))\n",
    "fac_scr_thr = nilearn.image.binarize_img(fac_scr_thr)\n",
    "\n",
    "joint_mask  = math_img('(in_sphere * face_responsive)', in_sphere=sphere_mask, face_responsive=fac_scr_thr)\n",
    "joint_mask_filename = os.path.join(fmri_results_dir,'sphere_and_faces-scrambled_Zmap_fdr.nii.gz')\n",
    "nibabel.save(joint_mask, joint_mask_filename)\n",
    "plot_roi(joint_mask, bg_img=mni152_template, title='Joint sphere and functional activation mask', display_mode='ortho', cmap='Set1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the mean values from a set of images\n",
    "\n",
    "Once the spherical ROI is created, we can extract and compute the mean activation values within this ROI across multiple images. Note the ROI and images must be in the same space (e.g., MNI-space here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_ROI_values(result_files, ROI):\n",
    "    \"\"\"\n",
    "    Extract mean ROI values from a list of results files using a provided ROI mask.\n",
    "\n",
    "    Parameters:\n",
    "    - result_files: List of BIDSImageFiles, or a list of file names, containing subject-level results.\n",
    "    - ROI: Nifti1Image containing the mask data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns: 'subject', 'condition', 'value'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the mask and the file have the same shape (dimensions) and resample if necessary\n",
    "    first_file = result_files[0]\n",
    "    if isinstance(first_file, str):\n",
    "        first_file = layout.get_file(first_file)\n",
    "    if not ROI.shape == first_file.get_image().shape[:3]:\n",
    "      ROI = resample_to_img(ROI, first_file.get_image(), interpolation='nearest')\n",
    "      print(f\"Resampled the mask to the shape of result files\")\n",
    "        \n",
    "    # Initialize an empty list to store the data\n",
    "    data_list = []\n",
    "\n",
    "    # Loop through the files and extract the mean ROI values\n",
    "    for file in result_files:\n",
    "        # If file is not a BIDSImageFile, but a string, load it\n",
    "        if isinstance(file, str):\n",
    "          file = layout.get_file(file)\n",
    "                 \n",
    "        # Get the subject ID\n",
    "        subject = file.get_entities()['subject']\n",
    "        \n",
    "        # Get the condition name\n",
    "        condition = file.get_entities()['desc']\n",
    "        \n",
    "        # Get the voxel values within the mask\n",
    "        img_data = file.get_image()\n",
    "        mask_data = ROI.get_fdata()\n",
    "        vox_values = img_data.get_fdata()[mask_data > 0]\n",
    "        # Compute the mean value\n",
    "        mean_value = numpy.mean(vox_values)\n",
    "        \n",
    "        # Append the data to the list\n",
    "        data_list.append([subject, condition, mean_value])\n",
    "    \n",
    "    # Convert the list to a pandas DataFrame\n",
    "    data = pandas.DataFrame(data_list, columns=['subject', 'condition', 'value'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the function to extract the mean contrast values within the ROI from each results file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_sphere = extract_mean_ROI_values(effect_files, sphere_mask)\n",
    "print(data_from_sphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An anatomical ROI from a probabilistic atlas (amygdala)\n",
    "\n",
    "In this section, we demonstrate how to create an ROI based on probabilistic data from the [Julich-Brain Cytoarchitectonic Atlas](https://search.kg.ebrains.eu/instances/ab191c17-8cd8-4622-aaac-eee11b2fa670). Probabilistic atlases provide voxel-wise information on the likelihood that a particular brain region corresponds to a given area, reflecting inter-individual variability in brain anatomy or function. The atlas is a single 4D image file, with each ROI stored as a separate volume in the file's fourth dimension. It is accompanied by a label file that indicates which volume corresponds to each region.\n",
    "\n",
    "You need to download these atlases into the `atlases` directory available from the course material here: [https://cloud.mrc-cbu.cam.ac.uk/index.php/s/3AFA0yF7QK9qIaO](https://cloud.mrc-cbu.cam.ac.uk/index.php/s/3AFA0yF7QK9qIaO) (though they can also be downloaded from other websites). The password for this cloud directory will be given by the course organiser.\n",
    "\n",
    "We selected the Julich-Brain Cytoarchitectonic Atlas for this analysis due to its comprehensive coverage of cytoarchitectonic regions, including the amygdala, which is of particular interest in the provided example. The choice of atlas often depends on the specific requirements of the study, such as the need for detailed cytoarchitectonic information or broader anatomical coverage.\n",
    "\n",
    "Here, we focus on generating an ROI for the amygdala by extracting and combining relevant regions from the atlas, applying a probability threshold, and creating a binary mask for use in subsequent analyses.\n",
    "\n",
    "We start by specifying the location of the atlas's image and label files, and setting the probability threshold. \n",
    "\n",
    "When using probabilistic atlases to define ROIs, **the choice of threshold** can significantly affect the final ROI. The threshold determines the minimum probability that a voxel belongs to a particular region, so a higher threshold will result in a smaller, more conservative ROI, while a lower threshold will include more voxels and create a larger ROI. The choice of threshold depends on your specific research question, the size and function of the region you are studying, and the need for either more inclusive or more conservative ROIs. A moderate threshold (20-30%) is frequently used as a balance between inclusivity and specificity. This threshold will exclude voxels that are less likely to belong to the region while still capturing the bulk of the structure.\n",
    "\n",
    "It’s advisable to visually inspect the resulting ROIs at different thresholds to ensure they are biologically plausible and appropriate for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of the atlas and labels files\n",
    "atlas_julich_file = 'atlases/Julich/JULICH_BRAIN_CYTOARCHITECTONIC_MAPS_2_9_MNI152_2009C_NONL_ASYM.pmaps_resized.nii.gz'\n",
    "labels_julich_file = 'atlases/Julich/JULICH_BRAIN_CYTOARCHITECTONIC_MAPS_2_9_MNI152_2009C_NONL_ASYM.txt'\n",
    "\n",
    "# Set the probability threshold\n",
    "prob_threshold = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we identify which volumes in the atlas correspond to the amygdala, our region of interest. We do this by consulting the accompanying label text file, where each ROI in the atlas is assigned a map index. We search for all labels that contain 'amygdala' in their names and extract their map indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels txt file to dataframe\n",
    "labels_df = pandas.read_csv(labels_julich_file, sep=\" \", quotechar=\"'\", names=['mapindex', 'regionname'], skiprows=1)\n",
    "print(f\"Atlas labels:\\n{labels_df}\\n----------------------------------\")\n",
    "\n",
    "# find the indexes for regions containing amygdala\n",
    "mapindex_of_interest = labels_df[labels_df['regionname'].str.contains('Amygdala')].index\n",
    "print(f\"Labels of interest:\\n{labels_df.loc[mapindex_of_interest]}\\n----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a binary mask for the amygdala from probabilistic atlas data, applying a probability threshold to ensure that only regions with sufficient likelihood of corresponding to the amygdala are included in the final mask. We do it in the following steps:\n",
    "\n",
    "1. Load the 4D atlas image.\n",
    "2. Extract the specific 3D volumes corresponding to the amygdala.\n",
    "3. Threshold the selected data to create a binary mask for each 3D volume.\n",
    "4. Combine these 3D images by summing them.\n",
    "5. Binarise the combined 3D data to create the final binary mask for the amygdala.\n",
    "6. Convert the combined data, which is a NumPy array, into a 3D NIfTI image. The affine transformation matrix from the original atlas image is used to ensure the mask aligns correctly in the same spatial orientation as the atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4D atlas image\n",
    "atlas_img = nibabel.load(atlas_julich_file, mmap=True)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# Select the maps of interest (from the 4th dimension of the 4D image)\n",
    "selected_data = atlas_data[:, :, :, mapindex_of_interest]\n",
    "\n",
    "# Threshold the data\n",
    "thresholded_data = numpy.where(selected_data > prob_threshold, 1, 0)\n",
    "\n",
    "# Combine these 3D images by summing them\n",
    "new_3d_data = numpy.sum(thresholded_data, axis=-1)\n",
    "\n",
    "# Binarize the data\n",
    "new_3d_data = numpy.where(new_3d_data > 0, 1, 0)\n",
    "\n",
    "# Cast the data to int8 to avoid compatibility issues\n",
    "new_3d_data = new_3d_data.astype(numpy.uint8)\n",
    "\n",
    "# Create a new NIfTI image with the 3D data\n",
    "amygdala_mask = nibabel.Nifti1Image(new_3d_data, atlas_img.affine) # could set dtype here, but setting it above works better (no warnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the created amygdala ROI and save if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the amygdala mask\n",
    "plot_roi(\n",
    "  amygdala_mask, \n",
    "  title='Amygdala mask', \n",
    "  display_mode='ortho', \n",
    "  draw_cross=False, \n",
    "  annotate=True, \n",
    "  black_bg=True, \n",
    "  bg_img = mni152_template, \n",
    "  cmap='Set1')\n",
    "\n",
    "# Save the mask to a file\n",
    "nibabel.save(amygdala_mask, os.path.join(fmri_results_dir,'amygdala_mask.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once we have the ROI, we can extract the contrast values using the previously created `extract_mean_ROI_values` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_amygdala = extract_mean_ROI_values(effect_files, amygdala_mask)\n",
    "print(data_from_amygdala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect to note is that the dimensions of the created amygdala ROI differ from those of the results files. For data extraction to occur correctly, the dimensions of the two must match. We have addressed this in our `extract_mean_ROI_values` function. If the ROI's dimensions do not match those of the results file, the ROI is resampled to align with the results file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the amygdala mask and the result files\n",
    "print(\"Amygdala mask shape:\", amygdala_mask.shape[:3])\n",
    "print(\"Result files shape:\", effect_files[0].get_image().shape[:3])\n",
    "\n",
    "# Check the voxel dimensions of the amygdala mask\n",
    "vox_dims = numpy.diag(amygdala_mask.affine)[:3]\n",
    "print(f\"Voxel dimensions of the amygdala mask: {vox_dims}\")\n",
    "\n",
    "# Check the voxel dimensions of the result files\n",
    "vox_dims = numpy.diag(effect_files[0].get_image().affine)[:3]\n",
    "print(f\"Voxel dimensions of the result files: {vox_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming MNI-space ROI to native-space for native-space analysis\n",
    "\n",
    "In the final example, we demonstrate how to transform an ROI defined in MNI space (e.g., a spherical mask) into a subject's native space using a transformation matrix derived during preprocessing with fMRIPrep. Once transformed, the ROI can be used to extract activation values from the native-space data of individual subjects. Because the native data have not be resampled/interpolated into MNI space, they will be  (slightly) more accurate than performing the analysis on normalized data. (Better still is to define the ROIs in each subject's native space, which is normally a time-consuming manual process, but might be necessary for small brain nuclei for example.)\n",
    "\n",
    "In this example, we will use the ANTs Python package [ANTsPy](https://github.com/ANTsX/ANTsPy) to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the MNI sphere on subject's native space anatomical image\n",
    "sID = '15'\n",
    "sub_anat_file = layout.get(subject=sID, datatype='anat', suffix='T1w', extension='nii.gz', desc=None, return_type='file')[0]\n",
    "\n",
    "plot_roi(sphere_mask, bg_img=sub_anat_file, title=f\"{ROI_label} sphere mask in native space; incorrect\", display_mode='ortho', dim=-1, cmap='Set1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNI_to_native_xfm_path = layout.get(subject=sID, datatype='anat', extension='h5', to='T1w', return_type='file')[0]\n",
    "print(f\"Found the following transformation file: {MNI_to_native_xfm_path}\")\n",
    "\n",
    "# Apply the transformation to the sphere mask\n",
    "ROI_native_space = ants.apply_transforms(\n",
    "  fixed = ants.image_read(sub_anat_file), \n",
    "  moving = ants.image_read(sphere_mask_filename),\n",
    "  transformlist = MNI_to_native_xfm_path, \n",
    "  interpolator = 'nearestNeighbor')\n",
    "\n",
    "# Convert ants image to nibabel image\n",
    "ROI_native_space = nibabel.Nifti1Image(ROI_native_space.numpy(), nibabel.load(sub_anat_file).affine)\n",
    "\n",
    "# plot the ROI in native space\n",
    "plot_roi(ROI_native_space, bg_img=sub_anat_file, title=f\"{ROI_label} sphere mask in native space; correct\", display_mode='ortho', dim=-1, cmap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, when working in MNI space, a single ROI mask file is sufficient for the entire group since all subjects are aligned to the same space. However, in native space, each subject requires a unique ROI mask due to the individual differences in brain anatomy. As a result, the `extract_mean_ROI_values` function must be applied separately to each subject's results files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics and plotting of the ROI data\n",
    "\n",
    "Now that we have created the ROI masks and extracted the data, we can proceed with statistical analyses as we would with any other dataset. Below are some examples of the ROI analysis results for our example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test for Familiar vs Unfamiliar Faces in Amygdala\n",
    "\n",
    "The amygdala may not have been very obvious in the whole-brain analysis we did in the previous notebook. We can gain power by averaging over voxels within an ROI. Let's also average initial and immediate repetitions of famous faces, and of unfamiliar faces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous = data_from_amygdala[data_from_amygdala['condition'].isin(['IniFF'])].groupby('subject')['value'].mean()\n",
    "unfamiliar = data_from_amygdala[data_from_amygdala['condition'].isin(['IniUF'])].groupby('subject')['value'].mean()\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(famous, unfamiliar, alternative='greater')\n",
    "print(f\"Paired t-test results: t-statistic = {t_stat}, p-value = {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference is significant, and to see in which direction (i.e. does amygdala respond more to famous or unknown faces), we can plot the data as below (though one can observe a possible outlier subject who might be driving the effect, and could be worth investigating further):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_famous_unfamiliar = pandas.DataFrame({\n",
    "    'Condition': ['Famous'] * len(famous) + ['Unfamiliar'] * len(unfamiliar),\n",
    "    'Value': pandas.concat([famous, unfamiliar]),\n",
    "    'Subject': list(famous.index) + list(unfamiliar.index)\n",
    "})\n",
    "\n",
    "# Adjust the figure size to make it narrower\n",
    "plt.figure(figsize=(4, 4))  # Reduce the width (first number) to make the plot narrower\n",
    "\n",
    "# Plot the point plot with dodge to avoid overlap\n",
    "seaborn.violinplot(x='Condition', y='Value', data=data_famous_unfamiliar, linestyles='-', color='lightgrey')\n",
    "\n",
    "# Add lines\n",
    "for subject in data_famous_unfamiliar['Subject'].unique():\n",
    "    subject_data = data_famous_unfamiliar[data_famous_unfamiliar['Subject'] == subject]\n",
    "    plt.plot([0, 1], subject_data['Value'], color='gray', alpha=0.3)\n",
    "\n",
    "plt.ylabel('Beta estimate'); plt.title('Paired t-test: Famous vs Unfamiliar'); plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated-measures ANOVA in Right Fusiform ROI\n",
    "\n",
    "We now return to our functionally-defined, spherical ROI from the peak in right fusiform, which we showed in the previous notebook was more active for faces than scrambled faces. Note that there would be little pointing testing whether faces produce significantly higher activation than scrambled faces in this ROI - this would be double-dipping, ie biased statistics because we have already defined the ROI by the same contrast. However, we can test for orthogonal contrasts, such as whether this region shows repetition effects, or even repetition effects that vary by stimulus-type, because these contrasts are orthogonal to the main effect of faces versus scrambled faces that defined the ROI (orthogonal contrasts are ones whose dot product is zero, where the contrasts themselves are the product of the contrast weights and the regressors; this is true when the dot product of the contrast weights is zero, eg [1 1].[1 -1]' = 0, as is the dot product of the columns of the design matrix on which they apply, which is true for ANOVA models in which each condition is modelled separately, eg [1 0].[0 1]' = 0).\n",
    "\n",
    "Let's test the 3x2 interaction between stimulus-type (famous, unfamiliar or scrambled faces) and repetition (initial or immediate repeat). We did test this interaction F-contrast across the whole brain in the previous notebook, but nothing survived correction for multiple comparisons. But restricting our analysis to one ROI reduces the correction needed, increasing our power (e.g, we could divide our alpha value by just 2, given that we have tested one contrast for each of two ROIs here: amygdala and fusiform).\n",
    "\n",
    "First we'll gather the conditions needed for the 3x2 interaction (ie ignoring the delayed repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop delayed repeats to emulate contrast in previous notebook\n",
    "data = data_from_sphere[data_from_sphere['condition'].isin(['IniFF', 'IniUF', 'IniSF', 'ImmFF', 'ImmUF', 'ImmSF'])].copy() \n",
    "data = data.sort_values('condition')\n",
    "\n",
    "# define the 3 levels of the stimulus-type factor\n",
    "data['stimulus'] = 'famous'\n",
    "data.loc[data['condition'].isin(['IniUF','ImmUF']), 'stimulus'] = 'unfamiliar'\n",
    "data.loc[data['condition'].isin(['IniSF','ImmSF']), 'stimulus'] = 'scrambled'\n",
    "\n",
    "# define the 2 levels of the repetition factor\n",
    "data['repetition'] = 'initial'\n",
    "data.loc[data['condition'].isin(['ImmFF', 'ImmUF', 'ImmSF']), 'repetition'] = 'immediate repeat'\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the repeated measures ANOVA\n",
    "aovrm = AnovaRM(data, depvar='value', subject='subject', within=['stimulus', 'repetition'])\n",
    "anova_res = aovrm.fit()\n",
    "print(anova_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interaction is significant, though again, let's visualise it, this time with a more sophisticated plot: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A 'publication-ready' plot\n",
    "\n",
    "For the plotting function below, we first create difference scores of immediate repeats minus initial presentations (so-called \"repeptition suppression\", RS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to wide (to do subtractions) and reset the (subject) index\n",
    "data = data_from_sphere.copy() \n",
    "data_pivot = data.pivot(index='subject', columns='condition', values='value').reset_index()\n",
    "#print(data_pivot)\n",
    "\n",
    "# Get the repetition suppression effect for each repetition condition for each subject.\n",
    "data_pivot['RS_FF'] = data_pivot['ImmFF']-data_pivot['IniFF']\n",
    "data_pivot['RS_UF'] = data_pivot['ImmUF']-data_pivot['IniUF']\n",
    "data_pivot['RS_SF'] = data_pivot['ImmSF']-data_pivot['IniSF']\n",
    "\n",
    "# Reshape the DataFrame to long format\n",
    "df_rs = data_pivot.melt(id_vars=['subject'], \n",
    "                      value_vars=['RS_FF', 'RS_UF', 'RS_SF'], \n",
    "                      var_name='condition', \n",
    "                      value_name='repetition_effect')\n",
    "\n",
    "# Redo ANOVA and Tukey T-tests for plotting below\n",
    "aovrm = AnovaRM(df_rs, depvar='repetition_effect', subject='subject', within=['condition'])\n",
    "anova_res = aovrm.fit()\n",
    "#print(anova_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interaction provides evidence that repetition suppression from immediate repetition varies by stimulus-type overall (e.g., appearing smaller for scrambled faces). We could explore further by testing all pairwise comparisons, using for example Tukey's method for correcting for multiple comparions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey_res = pairwise_tukeyhsd(endog=df_rs['repetition_effect'], groups=df_rs['condition'], alpha=0.05)\n",
    "print(tukey_res) # Tukey’s HSD: This test compares all possible pairs of means and controls the family-wise error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However testing all possible pairwise comparisons is rarely appropriate, and here we have planned comparisons of faces vs scrambled and famous vs unfamiliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Planned Comparisons (on repetition effects):\n",
    "contrasts = numpy.array([[0.5,  0.5, -1.0],    # is repetition effect bigger for faces than scrambled\n",
    "                         [1.0, -1.0,  0.0]])   # is repetition effect bigger for famous than unfamiliar\n",
    "dat = data_pivot[list(['RS_FF','RS_UF','RS_SF'])].to_numpy()\n",
    "\n",
    "for c in range(len(contrasts)):\n",
    "    contrast_dat = numpy.dot(dat, contrasts[c]) # Multiple data by contrast\n",
    "    res = stats.ttest_1samp(contrast_dat, 0, alternative='two-sided') # One-sample T-test on result\n",
    "    print(f\"Contrast {str(contrasts[c])}: \\tT({res.df})={numpy.round(res.statistic,3)}, p={numpy.round(res.pvalue,3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have evidence that repetition suppression (negative values for immediate repetition minus initial presentation) is greater for faces (averaged over famous and unfamiliar) than for scrambled faces in this ROI (but no evidence that repetition suppression differs for famous and unfamiliar faces).\n",
    "\n",
    "Here is an example function for plotting a number of effects, together with pairwise differences between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_effects(data, subject_col, condition_col, value_col, \n",
    "                    anova_results, tukey_results, condition_order, \n",
    "                    roi_label='', mni_coord='', title='', xlabel='', ylabel='', xticks=[]):\n",
    "    \"\"\"\n",
    "    Plots the results of a 1-by-3 ANOVA with within-subject 95% confidence intervals and significance annotations.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data (long format).\n",
    "    - subject_col: Name of the column representing subjects.\n",
    "    - condition_col: Name of the column representing the condition.\n",
    "    - value_col: Name of the column representing the dependent variable.\n",
    "    - anova_results: ANOVA results object containing F and p values.\n",
    "    - tukey_results: Tukey's HSD test results containing p-values.\n",
    "    - condition_order: List specifying the order of conditions.\n",
    "    - roi_label: Label for the region of interest (ROI).\n",
    "    - mni_coord: MNI coordinates as a string.\n",
    "    - xlabel: Label for the x-axis.\n",
    "    - ylabel: Label for the y-axis.\n",
    "    - xticks: List of custom x-axis labels.\n",
    "    - title: Title for the plot.\n",
    "    \n",
    "    Returns:\n",
    "    - A plot of the ANOVA results with error bars and significance markers.\n",
    "    \"\"\"\n",
    "    # Get the ANOVA F-value and p-value for the plot title\n",
    "    F_value = anova_results.anova_table['F Value'].iloc[0]\n",
    "    \n",
    "    if anova_results.anova_table['Pr > F'].iloc[0] < 0.001:\n",
    "        p_value = \"< 0.001\"\n",
    "    else:\n",
    "        p_value = f\"= {anova_results.anova_table['Pr > F'].iloc[0]:.3f}\"\n",
    "\n",
    "    # Determine the significance markers for the pairs based on the Tukey's HSD test\n",
    "    annotations = ['***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else 'n.s.' for p in tukey_results.pvalues]\n",
    "    \n",
    "    # Look in Tukey's results for the order of the pairs to know which pairs corresponds to which p-value\n",
    "    pairs = [(2, 1), (2, 0), (1, 0)]  \n",
    "    \n",
    "    # Ensure 'subject' and 'condition' are categorical variables\n",
    "    data[subject_col] = data[subject_col].astype('category')\n",
    "    data[condition_col] = data[condition_col].astype('category')\n",
    "    \n",
    "    # Calculate the mean for each condition\n",
    "    means = data.groupby(condition_col, observed=True)[value_col].mean()\n",
    "    \n",
    "    # Subtract the participant's mean value from each score to calculate deviations\n",
    "    data['deviation'] = data[value_col] - data.groupby(subject_col, observed=True)[value_col].transform('mean')\n",
    "    \n",
    "    # Calculate the standard error of the mean for each condition\n",
    "    sem = data.groupby(condition_col, observed=True)['deviation'].std() / numpy.sqrt(data[subject_col].nunique())\n",
    "    \n",
    "    # Calculate the t-value for the 95% confidence interval\n",
    "    t_value = stats.t.ppf(1 - 0.025, data[subject_col].nunique() - 1)  # degrees of freedom = n-1\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    ci = sem * t_value\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    \n",
    "    # Create a point plot\n",
    "    seaborn.pointplot(data=data, x=condition_col, y=value_col, order=condition_order, capsize=.1, errorbar=None, color='black', markers='o', linestyles='-', estimator=numpy.mean)\n",
    "    \n",
    "    # Add error bars using the calculated within-subject CI\n",
    "    plt.errorbar(x=range(len(means)), y=means[condition_order], yerr=ci[condition_order], fmt='none', capsize=5, color='black')\n",
    "    \n",
    "    # Add individual data points\n",
    "    seaborn.stripplot(data=data, x=condition_col, y=value_col, order=condition_order, color='grey', alpha=0.6, jitter=True)\n",
    "    \n",
    "    # Annotate the plot with significance markers\n",
    "    y_max = data[value_col].max() + (data[value_col].max() * 0.01)  # Position above the highest point\n",
    "    y_range = data[value_col].max() + (data[value_col].max() * 0.02)  # Adjust the range to avoid overlapping\n",
    "    \n",
    "    for i, (pair, annotation) in enumerate(zip(pairs, annotations)):\n",
    "        x1, x2 = pair\n",
    "        y = y_max + (i * 0.02)  # Space annotations vertically\n",
    "        plt.plot([x1, x2], [y, y], color='black', lw=1.5)\n",
    "        plt.text((x1 + x2) * 0.5, y + 0.002, annotation, ha='center', va='bottom', color='black')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    plt.axhline(y=0, color='grey')\n",
    "    \n",
    "    # Remove plot borders\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    \n",
    "    # Remove tick marks but keep the labels\n",
    "    plt.tick_params(axis='both', which='both', length=0)  \n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(f'\\n{xlabel}')\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    # Custom x labels\n",
    "    plt.xticks(ticks=range(len(means)), labels=xticks)\n",
    "    \n",
    "    # Set title\n",
    "    plt.title(f\"{title} \\n F = {F_value:.2f}, p {p_value}; error bars: within-subj 95% CI\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call it to plot the three repetition effects in our ROI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_effects(\n",
    "    data=df_rs,\n",
    "    subject_col='subject',\n",
    "    condition_col='condition',\n",
    "    value_col='repetition_effect',\n",
    "    anova_results=anova_res,\n",
    "    tukey_results=tukey_res,\n",
    "    condition_order=['RS_FF', 'RS_UF', 'RS_SF'],\n",
    "    roi_label=ROI_label,\n",
    "    mni_coord=MNI_coord,\n",
    "    xlabel='Stimulus-type',\n",
    "    ylabel='Immediate Repeat - Initial', \n",
    "    xticks=['famous', 'unfamiliar', 'scrambled'], \n",
    "    title=f\"{ROI_label}, MNI = {MNI_coord}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the general pattern for the significant interaction is greater repetition suppression for faces than scrambled faces in this ROI. Note that we are not making any scientific claims about this - it is just for demonstration purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
